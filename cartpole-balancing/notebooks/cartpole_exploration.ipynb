{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d590575",
   "metadata": {},
   "source": [
    "# CartPole DQN Exploration Notebook\n",
    "\n",
    "This notebook provides an interactive environment for exploring the CartPole DQN model and experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091824f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "\n",
    "from dqn_agent import DQNAgent\n",
    "from mlflow_utils import MLflowTracker\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc8198",
   "metadata": {},
   "source": [
    "## 1. Environment Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2aa676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CartPole environment\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Max episode steps: {env._max_episode_steps}\")\n",
    "\n",
    "# Sample some observations\n",
    "print(\"\\nSample observations:\")\n",
    "for i in range(5):\n",
    "    obs, _ = env.reset()\n",
    "    print(f\"Reset {i+1}: {obs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a78e8d",
   "metadata": {},
   "source": [
    "## 2. Random Agent Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7befff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_random_agent(episodes=100):\n",
    "    \"\"\"Test random agent performance.\"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = env.action_space.sample()  # Random action\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "    \n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "# Test random agent\n",
    "random_rewards = test_random_agent(100)\n",
    "print(f\"Random agent average reward: {np.mean(random_rewards):.2f} Â± {np.std(random_rewards):.2f}\")\n",
    "print(f\"Random agent max reward: {np.max(random_rewards)}\")\n",
    "\n",
    "# Plot random agent performance\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(random_rewards)\n",
    "plt.title('Random Agent Episode Rewards')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(random_rewards, bins=20, alpha=0.7)\n",
    "plt.title('Random Agent Reward Distribution')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615dd08d",
   "metadata": {},
   "source": [
    "## 3. Load and Analyze MLflow Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d23cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "# Get experiment\n",
    "experiment_name = \"cartpole-dqn\"\n",
    "try:\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment:\n",
    "        print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "        print(f\"Experiment Name: {experiment.name}\")\n",
    "        print(f\"Artifact Location: {experiment.artifact_location}\")\n",
    "        \n",
    "        # Get all runs\n",
    "        runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "        print(f\"\\nNumber of runs: {len(runs_df)}\")\n",
    "        \n",
    "        if len(runs_df) > 0:\n",
    "            # Display run summary\n",
    "            print(\"\\nRun Summary:\")\n",
    "            display_cols = ['run_id', 'status', 'start_time', 'metrics.summary_avg_reward', \n",
    "                          'params.learning_rate', 'params.episodes']\n",
    "            available_cols = [col for col in display_cols if col in runs_df.columns]\n",
    "            print(runs_df[available_cols].head())\n",
    "    else:\n",
    "        print(f\"Experiment '{experiment_name}' not found. Run training first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to MLflow: {e}\")\n",
    "    print(\"Make sure MLflow server is running: docker-compose up mlflow-server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f936832f",
   "metadata": {},
   "source": [
    "## 4. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot experiment comparison\n",
    "if 'runs_df' in locals() and len(runs_df) > 0:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Average rewards comparison\n",
    "    plt.subplot(1, 3, 1)\n",
    "    if 'metrics.summary_avg_reward' in runs_df.columns:\n",
    "        plt.bar(range(len(runs_df)), runs_df['metrics.summary_avg_reward'])\n",
    "        plt.title('Average Reward by Run')\n",
    "        plt.xlabel('Run Index')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.axhline(y=195, color='r', linestyle='--', label='Solved Threshold')\n",
    "        plt.legend()\n",
    "    \n",
    "    # Plot 2: Learning rate vs performance\n",
    "    plt.subplot(1, 3, 2)\n",
    "    if 'params.learning_rate' in runs_df.columns and 'metrics.summary_avg_reward' in runs_df.columns:\n",
    "        lr_vals = runs_df['params.learning_rate'].astype(float)\n",
    "        rewards = runs_df['metrics.summary_avg_reward']\n",
    "        plt.scatter(lr_vals, rewards)\n",
    "        plt.title('Learning Rate vs Average Reward')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.xscale('log')\n",
    "    \n",
    "    # Plot 3: Episodes to solve\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if 'metrics.episodes_to_solve' in runs_df.columns:\n",
    "        solve_episodes = runs_df['metrics.episodes_to_solve'].dropna()\n",
    "        if len(solve_episodes) > 0:\n",
    "            plt.hist(solve_episodes, bins=10, alpha=0.7)\n",
    "            plt.title('Episodes to Solve Distribution')\n",
    "            plt.xlabel('Episodes')\n",
    "            plt.ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601595a8",
   "metadata": {},
   "source": [
    "## 5. Load and Test Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb7036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_model():\n",
    "    \"\"\"Load the best trained model.\"\"\"\n",
    "    model_path = '../models/best_model.pth'\n",
    "    \n",
    "    try:\n",
    "        # Create agent\n",
    "        agent = DQNAgent(state_size=4, action_size=2)\n",
    "        \n",
    "        # Load model\n",
    "        checkpoint = agent.load_model(model_path)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "        print(f\"Training epsilon: {checkpoint['epsilon']:.4f}\")\n",
    "        \n",
    "        return agent\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Model file not found: {model_path}\")\n",
    "        print(\"Train a model first: python src/train.py\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load model\n",
    "trained_agent = load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fcf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trained_agent(agent, episodes=10, render=False):\n",
    "    \"\"\"Evaluate trained agent performance.\"\"\"\n",
    "    if agent is None:\n",
    "        print(\"No agent provided\")\n",
    "        return []\n",
    "    \n",
    "    env = gym.make('CartPole-v1', render_mode=\"human\" if render else None)\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while True:\n",
    "            action = agent.act(obs, training=False)  # No exploration\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode+1}: {total_reward} steps\")\n",
    "    \n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "# Evaluate trained agent\n",
    "if trained_agent:\n",
    "    trained_rewards = evaluate_trained_agent(trained_agent, episodes=5)\n",
    "    print(f\"\\nTrained agent average reward: {np.mean(trained_rewards):.2f}\")\n",
    "    print(f\"Trained agent max reward: {np.max(trained_rewards)}\")\n",
    "    \n",
    "    # Compare with random agent\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot([random_rewards[:50], trained_rewards], \n",
    "                labels=['Random Agent', 'Trained DQN'])\n",
    "    plt.title('Performance Comparison')\n",
    "    plt.ylabel('Episode Reward')\n",
    "    plt.axhline(y=195, color='r', linestyle='--', label='Solved Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698b163d",
   "metadata": {},
   "source": [
    "## 6. Action Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_agent_decisions(agent, num_samples=1000):\n",
    "    \"\"\"Analyze what actions the agent takes in different states.\"\"\"\n",
    "    if agent is None:\n",
    "        print(\"No agent provided\")\n",
    "        return\n",
    "    \n",
    "    env = gym.make('CartPole-v1')\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    q_values = []\n",
    "    \n",
    "    # Collect data\n",
    "    for _ in range(num_samples):\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        # Get action and Q-values\n",
    "        action = agent.act(obs, training=False)\n",
    "        \n",
    "        # Get Q-values\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(obs).unsqueeze(0).to(agent.device)\n",
    "            q_vals = agent.q_network(state_tensor).cpu().numpy()[0]\n",
    "        \n",
    "        states.append(obs)\n",
    "        actions.append(action)\n",
    "        q_values.append(q_vals)\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    q_values = np.array(q_values)\n",
    "    \n",
    "    # Analysis plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # State variable distributions by action\n",
    "    state_names = ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Velocity']\n",
    "    \n",
    "    for i, name in enumerate(state_names):\n",
    "        ax = axes[i//2, i%2]\n",
    "        \n",
    "        left_states = states[actions == 0, i]\n",
    "        right_states = states[actions == 1, i]\n",
    "        \n",
    "        ax.hist(left_states, bins=30, alpha=0.5, label='Left (0)', density=True)\n",
    "        ax.hist(right_states, bins=30, alpha=0.5, label='Right (1)', density=True)\n",
    "        ax.set_title(f'{name} Distribution by Action')\n",
    "        ax.set_xlabel(name)\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Q-value analysis\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(q_values[:, 0], q_values[:, 1], alpha=0.5)\n",
    "    plt.xlabel('Q-value for Left (0)')\n",
    "    plt.ylabel('Q-value for Right (1)')\n",
    "    plt.title('Q-value Correlation')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', alpha=0.5)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    q_diff = q_values[:, 1] - q_values[:, 0]  # Right - Left\n",
    "    plt.hist(q_diff, bins=50, alpha=0.7)\n",
    "    plt.xlabel('Q-value Difference (Right - Left)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Action Preference Distribution')\n",
    "    plt.axvline(x=0, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return states, actions, q_values\n",
    "\n",
    "# Analyze agent decisions\n",
    "if trained_agent:\n",
    "    states, actions, q_values = analyze_agent_decisions(trained_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21bc15",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f811d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze hyperparameter effects (if multiple runs exist)\n",
    "if 'runs_df' in locals() and len(runs_df) > 1:\n",
    "    \n",
    "    # Convert relevant columns to numeric\n",
    "    numeric_params = ['learning_rate', 'batch_size', 'epsilon_decay']\n",
    "    numeric_metrics = ['summary_avg_reward', 'summary_max_reward', 'episodes_to_solve']\n",
    "    \n",
    "    for param in numeric_params:\n",
    "        col_name = f'params.{param}'\n",
    "        if col_name in runs_df.columns:\n",
    "            runs_df[col_name] = pd.to_numeric(runs_df[col_name], errors='coerce')\n",
    "    \n",
    "    for metric in numeric_metrics:\n",
    "        col_name = f'metrics.{metric}'\n",
    "        if col_name in runs_df.columns:\n",
    "            runs_df[col_name] = pd.to_numeric(runs_df[col_name], errors='coerce')\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    param_cols = [f'params.{p}' for p in numeric_params if f'params.{p}' in runs_df.columns]\n",
    "    metric_cols = [f'metrics.{m}' for m in numeric_metrics if f'metrics.{m}' in runs_df.columns]\n",
    "    \n",
    "    if param_cols and metric_cols:\n",
    "        corr_data = runs_df[param_cols + metric_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        import seaborn as sns\n",
    "        sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0)\n",
    "        plt.title('Hyperparameter-Performance Correlation')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Performance over time\n",
    "    if 'start_time' in runs_df.columns and 'metrics.summary_avg_reward' in runs_df.columns:\n",
    "        runs_df['start_time'] = pd.to_datetime(runs_df['start_time'])\n",
    "        runs_sorted = runs_df.sort_values('start_time')\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(runs_sorted['start_time'], runs_sorted['metrics.summary_avg_reward'], 'o-')\n",
    "        plt.title('Performance Over Time')\n",
    "        plt.xlabel('Experiment Time')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.axhline(y=195, color='r', linestyle='--', label='Solved Threshold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Not enough runs for hyperparameter analysis. Run multiple experiments with different parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d371a052",
   "metadata": {},
   "source": [
    "## 8. Next Steps and Experiments\n",
    "\n",
    "Based on this analysis, here are some experiments you can try:\n",
    "\n",
    "1. **Hyperparameter Tuning**: Experiment with different learning rates, batch sizes, and network architectures\n",
    "2. **Algorithm Variations**: Try Double DQN, Dueling DQN, or Rainbow DQN\n",
    "3. **Different Environments**: Apply the same code to other Gym environments\n",
    "4. **Curriculum Learning**: Start with easier tasks and gradually increase difficulty\n",
    "5. **Model Interpretability**: Analyze what features the network learns\n",
    "\n",
    "### Quick Experiment Commands:\n",
    "\n",
    "```bash\n",
    "# Try different learning rates\n",
    "python src/train.py --lr 0.0001 --episodes 500\n",
    "python src/train.py --lr 0.001 --episodes 500\n",
    "python src/train.py --lr 0.01 --episodes 500\n",
    "\n",
    "# Try different batch sizes\n",
    "python src/train.py --batch-size 16 --episodes 500\n",
    "python src/train.py --batch-size 64 --episodes 500\n",
    "python src/train.py --batch-size 128 --episodes 500\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
